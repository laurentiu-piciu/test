{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAPORT ACTIVITATE IUNIE 2017-SEPTEMBRIE 2017\n",
    "\n",
    "- <i><b><a href=\"mailto:laurentiu.piciu@4esoft.ro\" style=\"color: #848484\">Laurentiu-Gheorghe PICIU</a></b><font color='#848484'> (4E Software & Facultatea de Automatică și Calculatoare - UPB)</i> <br>\n",
    "- <i><b><a href=\"mailto:andrei.simion@4esoft.ro\" style=\"color: #848484\">Andrei SIMION-CONSTANTINESCU</a></b><font color='#848484'> (4E Software & Facultatea de Automatică și Calculatoare - UPB)</i>\n",
    "\n",
    "## Prezentare generală\n",
    "\n",
    "În domeniul Învățării Automate (Machine Learning), orice problemă are ca punct de plecare existența unor date. O pereche de forma $(x^{(i)}, y^{(i)})$ constituie un exemplu din setul de date, unde $x^{(i)}$ reprezintă variabilele de intrare (predictori), iar $y^{(i)}$ reprezintă variabila target. Astfel, dându-se un set de date, scopul este să se găsească o funcție $h: X \\rightarrow Y$, cu proprietatea ca $h(x)$ este un predictor bun pentru valoarea corespunzatoare - $y$. În momentul în care varibila target este continuă, atunci interacționăm cu o problemă de **regresie**. Altfel, dacă $y$ aparține unei mulțimi cu un număr mic de valori discrete, atunci problema este una de **clasificare**.\n",
    "\n",
    "Atât în cazul regresiei liniare, cât și în cazul regresiei logistice (clasificare), se dorește găsirea unui **hiperplan** care să aproximeze cât mai bine punctele din setul de date. Ecuația hiperplanului este data de înmulțirea dintre parametrii $\\theta$ (pe care dorim să îi îmbunătățim prin antrenament) și predictori:\n",
    "\n",
    "$h_{\\theta}(x) = \\theta_{0} + \\theta_{1}x_{1} + \\theta_{2}x_{2} + ... + \\theta_{n}x_{n} = \\begin{bmatrix} \\theta_{0} &  \\theta_{1} & \\theta_{2} & ... & \\theta_{n}\\end{bmatrix}\\begin{bmatrix} x_{0} \\\\ x_{1} \\\\ x_{2} \\\\ .. \\\\ x_{n}\\end{bmatrix} = \\theta^Tx$, unde $n = $numărul predictorilor și $x_{0} = 1$.\n",
    "\n",
    "1. <h6>Problema clasificării</h6>\n",
    "Pentru ca funcția ipoteză să întoarcă valori în intervalul $[0, 1]$ s-a introdus funcția **sigmoid** $g(z) = \\frac{1}{1 + e^{-z}} \\implies h_{\\theta}(x) = g(\\theta^Tx)$.\n",
    "![alt text](sigmoid.gif \"Graficul funcției sigmoid.\")\n",
    "\n",
    "    Funcția sigmoid este de ajutor pentru clasificare binară, dar și pentru clasificare multinomială **One-vs-All**. Pentru generalizarea clasificării multinomiale, se folosește funcția **softmax**, care este o generalizare a sigmoid-ului. Pentru a putea fi folosită funcția softmax, trebuie ca variabila target sa fie **one-hot**, astfel încât funcția softmax să ofere K probabilități (K = numărul de clase):\n",
    "    \n",
    "    $x_{\\theta}(x^{(i)}) = \\begin{bmatrix} p(y^{(i)} = 1\\,|\\,x^{(i)}; \\theta) \\\\ p(y^{(i)} = 2\\,|\\,x^{(i)}; \\theta)  \\\\ .. \\\\ p(y^{(i)} = k\\,|\\,x^{(i)}; \\theta)\\end{bmatrix} = \\frac{1}{\\sum_{j=1}^{k}e^{\\theta_j^Tx^{(i)}}}\\begin{bmatrix} e^{\\theta_1^Tx^{(i)}} \\\\ e^{\\theta_2^Tx^{(i)}}  \\\\ .. \\\\ e^{\\theta_k^Tx^{(i)}}\\end{bmatrix}$\n",
    "\n",
    "2. <h6>Funcția de cost</h6>\n",
    "Măsurarea performanțelor unui model se face cu ajutorul **funcției de cost**. Pentru clasificare se folosește funcția de cost **cross-entropy**.\n",
    "\n",
    "    $J(\\theta) = \\frac{1}{m}\\sum_{i=1}^{m}Cost(h_{\\theta}(x^{(i)}), y^{(i)})$, unde $m = $numărul de observații. <br>\n",
    "    $Cost(h_{\\theta}(x), y) = -log(h_{\\theta}(x))$, dacă $y = 1$ <br>\n",
    "    $Cost(h_{\\theta}(x), y) = -log(1 - h_{\\theta}(x))$, dacă $y = 0$ <br><br>\n",
    "    $\\implies J(\\theta) = -\\frac{1}{m}\\sum_{i=1}^{m}[y^{(i)}log(h_{\\theta}(x^{(i)})) + (1 - y^{(i)})log(1 - h_{\\theta}(x^{(i)}))]$\n",
    "\n",
    "2. <h6>Tunarea parametrilor</h6>\n",
    "Pentru îmbunătățirea parametrilor, se va ține cont de derivatele parțiale ale funcției de cost în funcție de fiecare parametru $\\theta_j$, aplicându-se metoda **gradientului descendent**, care modifică parametrii astfel încât funcția de cost să descrească.\n",
    "\n",
    "    $Repeta\\;\\{ \\\\\n",
    "    \\quad \\theta_j := \\theta_j - \\alpha\\frac{\\partial}{\\partial\\theta_j}J(\\theta)\\\\\n",
    "    \\}$ <br><br>\n",
    "    \n",
    "    $Repeta\\;\\{ \\\\\n",
    "    \\quad \\theta_j := \\theta_j - \\frac{\\alpha}{m}\\sum_{i=1}^{m}(h_{\\theta}(x^{(i)} - y^{(i)})x_j^{(i)} \\\\\n",
    "    \\}$\n",
    "\n",
    "În această perioadă, activitatea noastră a avut ca scop înțelegerea conceptelor care stau în spatele modelelor de regresie/clasificare, precum și construirea unor astfel de modele. În principal, tot ceea ce s-a construit, a avut ca punct de plecare setul de date **MNIST**, care conține 70,000 de imagini de dimensiune 28x28, reprezentând cifrele de la 0 la 9 scrise de mână. Pe baza acestor imagini, s-au antrenat modele de clasificare din ce în ce mai complexe, pornind de la **regresie logistică**, **KNN** și **SVM**, avansând la **rețele neuronale complet conectate (fully-connected)** și terminând cu **rețele neuronale de convoluție (CNN)**. Pentru fiecare model, setul de date a fost împărțit în date de antrenare (70%), date de validare în timpul antrenamentului (15%), precum și date de testare la finalul antrenamentului (15%). În plus, varibilele predictor (cei 784 de pixeli) au fost scalate între 0 și 1, folosind **scalarea (normalizarea) Min-Max**, ce face ca variabilele să \"cântărească\" la fel de mult și, în plus, crește abilitatea modelului de a învăța. Formula folosită pentru normalizare este: $X_{norm} = \\frac{X - X_{min}}{X_{max}-X_{min}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<script src=\"https://gist.github.com/jonschlinkert/5854601.js\"></script>\n",
    "# Conținut\n",
    "  1. [<font color='black'>Regresie logistică</font>](#chapter-1)\n",
    "  2. [<font color='black'>KNN</font>](#chapter-2)\n",
    "  3. [<font color='black'>SVM</font>](#chapter-3)\n",
    "  4. [<font color='black'>Rețele neuronale complet conectate</font>](#chapter-4)\n",
    "  5. [<font color='black'>Rețele neuronale de convoluție (CNN)</font>](#chapter-5)\n",
    "  6. [<font color='black'>Fereastră glisantă</font>](#chapter-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresie logistică\n",
    "<a id='chapter-1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN\n",
    "<a id='chapter-2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM\n",
    "<a id='chapter-3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Rețele neuronale complet conectate\n",
    "<a id='chapter-4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rețele neuronale de convoluție (CNN)\n",
    "<a id='chapter-5'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fereastră glisantă\n",
    "<a id='chapter-6'></a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
